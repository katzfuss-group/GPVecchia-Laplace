VL Progress


To Do
----------------------------
-SPDE from INLA to replace Vecchia
-describe in more detail the effect of a nonzero mean for latent gp
-CI for both pseudodata and posterior
-wrote comments for signal to noise; elaborate?
-plots for simulations:
	1.  1D (redo)
	2.  2D (rerunning, then redo)
	3.  1D, large sample size timing. (rerun)
	4.  HMC comparison (rerun)
	5.  parameter estimation proceedure for VL approaches parameter 	estimation for L as neighbor count increases










Side quest
----------------------------
-Collect extra material into supplementary doc
-timing for large sample size -  check C++
-try making a package
-error of VL:  asymptotic error for misspecified model






============================================
 Tues March 13

-Collect extra material into supplementary doc
-try making a package
-CI for both pseudodata and posterior
-timing for large sample size -  check C++


var covar/ var nugget:  get signal to noise via pseudo-data nugget-->. what is pseudo-data signal to noise; eg if binary and highly mixed, is that equivalent to more noise?  Poisson 
--visualize pseudo-data:  posterior mean, intervals. (show)

--Try getting sparse precision from SPDE from INLA. (read about triangulation, not yet at parameter estimation -  nu in Z+, Z+.5+)
		-Try SPDE substitute for Vecchia

--run sim again (running!)





--------------------------


-----gaussian: iteration count, with more noise (signal to noise, informative data) (no difference)
-non-zero mean:  easy to adjust model?  propagate through models
-parameter estimation for large number of nbrs approaches laplace
	-for gaussian and nongaussian --> approach laplace

-new code has issue for low rank where knots are somewhat random.  Goes away with larger m (~10) 

check sheth chen for prior mean != 0. ("Assume 0 for simplicity")

Section to describe prediction?  Seems obvious:  for z*, find y* then pass through link function


------------------

-standardize appearance of plots:  low is better
-Pure latent SGV:  faster convergence to Laplace with neighbor count
-STAN, time plot (baed)

-update paper according to edits
-Consider centering for non-gaussian likleihoods:  if mean >0, what is affected - 
-Rue 2009 INLA paper:  comment for convergence of laplace, discussion?
-update/check count of iterations for running VL

-asymptotic error rate for misspecified model?

-checked iterations for n=500 (5), 1k(5) , 2k(5) , 10k (5) ->  seems to be independent of sample size;  Gaussian < logistic  < Poisson + Gamma



------------
-

-Log score is strictly proper, averaging ok [Gneiting Katzfuss 2014 Prob. Forecasting], Annular 





-parallelize code (done)

-parameter estimation by hand (done)

----

-running code on server;  not taking advantage of parallellization

-convergence definitely holds, need to re-run sample
- consider theoretical comparison?  MSE from NR hard to work with. 


-log score: average discrepancy between laplace and others, rather than avg value:  value changes significantly between seed values for generated data

-efficiency vs time:  conditioning points and overhead  (Onm^3)

-FSA: better comparison rather than MPP? --> Scaling issues, think infill asymptotics

-----------

-Check conditioning set size up to n for MSE and logscore: should converge
-parameter estimation with gaussian
-matern range vs effective range; for 2D check for eff range ~ .2
-LR with HMC:  
-rewrite proof with GP prior
-fixed iteration comparison between HMC and L:  HMC works poorly (MSE, Logscore) with 1000 iterations vs 60 of L/VL

-SPDE (R package) for 3d


-----
-set up maximin distance
-implemented Gamma
-set up HMC to run with sample generators
-Log score convergence in 1D is fast, 2D is slow
-comment about shape vs scale param:  identifiability











-degrees of freedom and variance,  etc